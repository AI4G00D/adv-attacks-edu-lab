# ğŸ” adv-attacks-edu-lab

**Educational Lab for Adversarial Machine Learning (ML) and Large Language Model (LLM) Attacks**  
*â€” with strong emphasis on AI safety, ethical demonstration, and responsible use.*

---

## ğŸ¯ Purpose

This repository is a curated educational playground showcasing adversarial attacks in both traditional ML and LLM systems. It aims to help students, security researchers, and AI developers understand how models can be manipulatedâ€”and how to defend against such threats.

> âš ï¸ All experiments and demos are conducted in isolated environments and are intended strictly for **educational purposes**.

---

## ğŸ§ª Attack Categories Covered

| Category                  | Description                                                                 |
|---------------------------|-----------------------------------------------------------------------------|
| `prompt-injection-playground/` | Demonstrates how prompts can be manipulated to override LLM safety instructions |
| `fgsm-attack/`            | Uses Fast Gradient Sign Method to perturb image classifiers (ML)            |
| `pgd-attack/`             | Projects adversarial noise to trick neural nets (ML)                        |
| `jailbreak-attacks/`      | Bypasses LLM content filters to produce unsafe or forbidden outputs         |
| `data-poisoning-attacks/` | Inserts corrupted samples in training data to degrade or subvert behavior   |
| _(more coming soon...)_   |                                                                             |

---

## ğŸ“¦ Repository Structure

```plaintext
adv-attacks-edu-lab/
â”‚
â”œâ”€â”€ README.md                  â† You are here
â”œâ”€â”€ LICENSE                    â† MIT License for educational reuse
â”‚
â”œâ”€â”€ prompt-injection-playground/
â”‚   â””â”€â”€ README.md, .ipynb, demo video
â”‚
â”œâ”€â”€ fgsm-attack/
â”‚   â””â”€â”€ README.md, FGSM notebook
â”‚
â”œâ”€â”€ pgd-attack/
â”‚   â””â”€â”€ README.md, PGD notebook
â”‚
â””â”€â”€ ...
